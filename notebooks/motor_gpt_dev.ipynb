{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e23420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os, glob, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9d1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38ee33",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04df0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_dim, embed_dim, traj_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # linear projections for Q, V, K\n",
    "        self.key = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        mask = torch.tril(torch.ones(traj_size, traj_size)).view(1, traj_size, traj_size)\n",
    "        self.atten_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x) #(B, T, H)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  # (B, T, T)\n",
    "        att = att.masked_fill(self.mask[:,:T,:T]==0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)  # (B, T, T)\n",
    "        att = self.atten_drop(att)\n",
    "\n",
    "        out = att @ v  # (B, T, H)\n",
    "        out = self.resid_drop(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a523d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# test the SelfAttentionHead\n",
    "head = SelfAttentionHead(head_dim=16, embed_dim=32, traj_size=8)\n",
    "x = torch.randn(4, 8, 32)  # (B, T, C)\n",
    "out = head(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624f7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, head_dim, traj_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(head_dim, embed_dim, traj_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_dim, embed_dim, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        multi_head_out = [h(x) for h in self.heads]  # list of (B, T, head_size)\n",
    "        multi_head_concat = torch.cat(multi_head_out, dim=-1) # (B, T, num_heads * head_size)\n",
    "\n",
    "        out = self.drop(self.proj(multi_head_concat))  # (B, T, embed_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494fe180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(num_heads=4, embed_dim=32, head_dim=8, traj_size=8, dropout=0.0)\n",
    "x = torch.randn(4, 8, 32)\n",
    "out = mha(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657d299",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f51df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion*embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expansion*embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, n_head, block_size, mlp_expansion=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_head == 0\n",
    "        head_size = embed_dim // n_head\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(n_head, embed_dim, head_size, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = FeedForward(embed_dim, expansion=mlp_expansion, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = x + self.attn(self.ln1(x)) # skip connection\n",
    "        x = x + self.mlp(self.ln2(x)) # skip connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdac5e",
   "metadata": {},
   "source": [
    "### Motor GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dbaff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorGPT(nn.Module):\n",
    "    def __init__(self, action_size=6, embed_dim=192, traj_size=128, n_layer=4, n_head=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.action_size = action_size\n",
    "        self.traj_size = traj_size\n",
    "\n",
    "        # “token embedding” for continuous actions\n",
    "        self.token_emb = nn.Linear(action_size, embed_dim, bias=False)\n",
    "        self.pos_emb   = nn.Embedding(traj_size, embed_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, n_head, traj_size, dropout=dropout)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # regression head back to action space\n",
    "        self.head = nn.Linear(embed_dim, action_size, bias=True)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # NanoGPT init is fine conceptually, but add LayerNorm explicitly\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        x:       (B, T, A)  normalized actions\n",
    "        targets: (B, T, A)  next-step normalized actions\n",
    "        \"\"\"\n",
    "        B, T, A = x.shape\n",
    "        assert T <= self.traj_size, \"Sequence length exceeds traj_size\"\n",
    "\n",
    "\n",
    "        #token and position embedding\n",
    "        tok = self.token_emb(x)  # (B, T, E)\n",
    "        pos = self.pos_emb(torch.arange(T, device=x.device)).unsqueeze(0)  # (1, T, E)\n",
    "        h = tok + pos\n",
    "\n",
    "        #pass through Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "\n",
    "        # regression head \n",
    "        h = self.ln_f(h)\n",
    "        pred = self.head(h)      # (B, T, A)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.mse_loss(pred, targets)  # regression v1\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, seed_actions, max_new_steps=100, noise_std=0.0):\n",
    "        \"\"\"\n",
    "        seed_actions: (B, T0, A) normalized actions\n",
    "        returns:      (B, T0+max_new_steps, A) normalized actions\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        out = seed_actions\n",
    "        for _ in range(max_new_steps):\n",
    "            cond = out[:, -self.traj_size:, :]      # crop context, so only last traj_size steps\n",
    "            pred, _ = self(cond)                    # (B, Tcond, A), this includes the whole context\n",
    "            next_a = pred[:, -1, :]                 # (B, A), take only the last time step\n",
    "\n",
    "            # optional stochasticity (since MSE tends to be “average”)\n",
    "            if noise_std > 0:\n",
    "                next_a = next_a + noise_std * torch.randn_like(next_a)\n",
    "\n",
    "            out = torch.cat([out, next_a.unsqueeze(1)], dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30310dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model, train_eps, val_eps, train_wlens, val_wlens,\n",
    "                  trajectory_len, batch_size, eval_iters, device):\n",
    "    # samples random batches from train and val sets, computes mean loss\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    with torch.no_grad():\n",
    "        for name, eps, wl in [(\"train\", train_eps, train_wlens), (\"val\", val_eps, val_wlens)]:\n",
    "            losses = []\n",
    "            for _ in range(eval_iters):\n",
    "                xb, yb = get_batch_train(eps, wl, trajectory_len, batch_size, device)\n",
    "                _, loss = model(xb, yb)\n",
    "                losses.append(loss.item())\n",
    "            out[name] = float(np.mean(losses))\n",
    "    model.train()\n",
    "    return out # returns {\"train\": mean_loss, \"val\": mean_loss}\n",
    "\n",
    "\n",
    "def save_checkpoint(path, model, optimizer, step, best_val=None, metadata=None):\n",
    "    # builds a directory and saves model and optimizer state dicts\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"step\": step,\n",
    "        \"best_val\": best_val,\n",
    "        \"metadata\": metadata or {},\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def train_model(model, train_eps, val_eps, train_wlens, val_wlens,\n",
    "                trajectory_len=128, batch_size=64, max_iters=2000,\n",
    "                eval_interval=100, lr=3e-4, weight_decay=0.1,\n",
    "                grad_clip=1.0, device=\"cuda\", checkpoint_dir=None,\n",
    "                save_interval=None, save_best=True, metadata=None):\n",
    "\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.95))\n",
    "\n",
    " \n",
    "    if checkpoint_dir is not None:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        if save_interval is None:\n",
    "            save_interval = eval_interval\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    training_losses, validation_losses, eval_steps = [], [], []\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        # sample batch, compute loss\n",
    "        xb, yb = get_batch_train(train_eps, train_wlens, trajectory_len, batch_size, device)\n",
    "        _, loss = model(xb, yb)\n",
    "\n",
    "        # backprop and update\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "\n",
    "        #periodic evaluation\n",
    "        if it % eval_interval == 0 or it == max_iters - 1:\n",
    "            est = estimate_loss(model, train_eps, val_eps, train_wlens, val_wlens,\n",
    "                                trajectory_len, batch_size, eval_iters=25, device=device)\n",
    "            print(f\"iter {it:5d} | train {est['train']:.6f} | val {est['val']:.6f}\")\n",
    "            training_losses.append(est[\"train\"])\n",
    "            validation_losses.append(est[\"val\"])\n",
    "            eval_steps.append(it)\n",
    "\n",
    "            if checkpoint_dir is not None:\n",
    "                if save_best and est[\"val\"] < best_val:\n",
    "                    best_val = est[\"val\"]\n",
    "                    best_path = os.path.join(checkpoint_dir, \"best.pt\")\n",
    "                    save_checkpoint(best_path, model, opt, it, best_val=best_val, metadata=metadata)\n",
    "\n",
    "                if it % save_interval == 0 or it == max_iters - 1:\n",
    "                    ckpt_path = os.path.join(checkpoint_dir, f\"ckpt_{it:06d}.pt\")\n",
    "                    save_checkpoint(ckpt_path, model, opt, it, best_val=best_val, metadata=metadata)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(eval_steps, training_losses, label=\"train\")\n",
    "    plt.plot(eval_steps, validation_losses, label=\"val\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.title(\"MotorGPT Train/Val Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
